import urllib2, urllib, cookielib
from bs4 import BeautifulSoup
import requests
from urlparse import urljoin
import argparse
from collections import OrderedDict
from distutils.tests.setuptools_build_ext import if_dl
import re
import sys
import Flg_Crawler_Step1

urls_nvisited = []

# login to the website
def send_login_request(input_url,url_postrequest,form_field_dict,urls_visited,cookie_dict= {},referer= False,origin=False,useragent=False):
    new_cookie = []
    urls_incurrentpage = []
    if not bool(cookie_dict):
        response = requests.post(url_postrequest,form_field_dict,verify=False,headers = {'UserAgent':useragent})
    else:
        response = requests.post(url_postrequest,form_field_dict,verify=False,cookies=cookie_dict)
    page_content = response.text
    html_pagecontent = BeautifulSoup(page_content)
    atags_incurrentpage = html_pagecontent.find_all('a')
    for url_incurrentpage in atags_incurrentpage:
        if ('href' in dict(url_incurrentpage.attrs)):
            urls_incurrentpage.append(urljoin(input_url, url_incurrentpage['href']))
    for new_link in urls_incurrentpage:
        if not new_link in urls_visited:
            urls_nvisited.append(new_link)
    print(response.text)
    response_url = response.url
    if not response.history:
        for c in response.cookies:
            cookie_dict[c.name] = c.value
    else:
        for response in response.history:
            for c in response.cookies:
                cookie_dict[c.name] = c.value
        response.history= []
    return (response_url,response.text,urls_nvisited,cookie_dict)
    
def crawl_after_login(location,response_url,forbiddenurls,logincookie,referer,urls_visited,urls_nvisited=[]):
    urls_visitedbefore, postrequest_inurls, cookie_value = Flg_Crawler_Step1.crawl_website(location,response_url,forbiddenurls,cookie_dict=logincookie,referer=referer,urls_visited=urls_visited,urls_nvisited=urls_nvisited)
    return cookie_value
