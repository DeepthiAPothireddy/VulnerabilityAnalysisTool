import urllib2, urllib, cookielib
from bs4 import BeautifulSoup
import requests
from urlparse import urljoin
import argparse
from collections import OrderedDict
from distutils.tests.setuptools_build_ext import if_dl
import re
import sys
import Flg_Crawler_Step1, Flg_Crawler_Step2,Flg_Crawler_xss_check

cookie_dict = {}

# parsing arguments through commandline
def parse_arguments():
    parser = argparse.ArgumentParser(description='Parse website and find vulnerabilites')
    parser.add_argument('-u', help='url link', required=True)
    parser.add_argument('-r', help='referer', required=False)
    parser.add_argument('-o', help='origin', required=False)
    parser.add_argument('-c', help='cookies', nargs='+')
    parser.add_argument('-ua', help='useragent',nargs='+')
    parser.add_argument('-of', help='location of outputfiles',nargs='+',required=True)
    parser.add_argument('-l',help='enter login url',required = True)
    args = parser.parse_args()
    return args

# First pass through the program. Access all open pages.
login_cookie = {}
cookie_val = False
# add command arguments to variables        
arguments_set = parse_arguments()
input_url = arguments_set.u
login_url = arguments_set.l
referer = arguments_set.r
origin = arguments_set.o
cookie_args = arguments_set.c
if cookie_args:
    for i in range(len(cookie_args)):
        cookie_list = cookie_args[i].split('=')
        cookie_dict[cookie_list[0]]= cookie_list[1]
useragent_args = arguments_set.ua
if not useragent_args:
    useragent = "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36"
else:
    useragent = ''.join(useragent_args) 
postrequest_args = arguments_set.of
if postrequest_args:
    location = ' '.join(postrequest_args)
postrequest_file = location+'/output.txt'
with open(postrequest_file, "w"):
    pass
postrequesttype_file = location+"/postrequesttype.txt"
with open(postrequest_file, "w"):
    pass

# call crawl first time
forbiddenurls = raw_input("Enter pages you do not want to crawl ")
urls_visitedbefore, postrequest_inurls, cookie_value = Flg_Crawler_Step1.crawl_website(location,input_url,forbiddenurls) 
urls_visited = []
# login to the page and call crawl after post request
# Second pass through the program. If there are postrequest pages.
with open(postrequest_file) as postrequest_fileobject:
    postrequestlist_infile = postrequest_fileobject.readlines()
for i in range(len(postrequestlist_infile)):
    required_line = postrequestlist_infile[i].split(',')
    print(required_line[0])
    url_postrequest = required_line[0]
    if not url_postrequest in urls_visited:
        if login_url == url_postrequest:
            form_field_dict = OrderedDict((el,0) for el in required_line[1:-1])
            for key in form_field_dict:
                print("form field name: " + key)
                form_field_dict[key] = raw_input("Enter value ")
            response_url,response_text,urls_nvisited,login_cookie = Flg_Crawler_Step2.send_login_request(input_url,url_postrequest, form_field_dict,urls_visited,cookie_dict=cookie_dict,referer=referer,origin=origin,useragent=useragent)
            login_success = raw_input("Was login successful? Enter Y or N")
            if ('Y' in login_success) or ('y' in login_success):
                urls_visited.append(url_postrequest)
                cookie_val = Flg_Crawler_Step2.crawl_after_login(location,response_url,forbiddenurls,login_cookie,referer,urls_visited,urls_nvisited)
            else:
                sys.exit("login failed. Please try to run tool again.") 
                
# XSS check
if not cookie_val:
    Flg_Crawler_xss_check.check_ifxsspsbl(login_url,postrequest_file,postrequesttype_file,location)
else:
    Flg_Crawler_xss_check.check_ifxsspsbl(login_url,postrequest_file,postrequesttype_file,location,cookie_val)
